# 数据分析常用算法



[TOC]



### SVM、LR、决策树的对比？（经常问）

模型复杂度：SVM支持核函数，可处理线性、非线性问题；LR模型简单，训练速度快，适合处理线性问题；决策树容易过拟合，需要进行剪枝
损失函数：SVM **铰链损失**（hinge loss）; LR L2正则化; adaboost 指数损失
数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

损失函数：https://blog.csdn.net/u010976453/article/details/78488279

逻辑回归（LR）和支持向量机（SVM）的区别和联系：https://www.cnblogs.com/huangyc/p/9943364.html#_label3

LR,SVM,RF的区别：https://blog.csdn.net/zhangbaoanhadoop/article/details/82055643

补充：

**LR和SVM的联系**

1. 都是监督学习的分类算法。
2. 都是线性分类方法 (不考虑核函数时）。
3. 都是[判别模型](https://www.cnblogs.com/huangyc/p/9943364.html#_label3)。

**LR和SVM的不同**

1. [损失函数](https://www.cnblogs.com/huangyc/p/9938314.html)的不同，**LR是对数损失函数，SVM是hinge损失函数**。
2. SVM不能产生概率，LR可以产生概率。
3. SVM自带**结构风险最小化**，LR则是**经验风险最小化**。
4. SVM会用核函数而LR一般不用[核函数](https://www.cnblogs.com/huangyc/p/9940487.html)。
5. LR和SVM在实际应用的区别：根据经验来看，对于**小规模数据集**，SVM的效果要好于LR，但是**大数据中**，SVM的计算复杂度受到限制，而LR因为训练简单，可以在线训练，所以经常会被大量采用。

**概念解释**

- **判别模型**：是直接生成一个表示或者的判别函数（或预测模型），SVM和LR，KNN，决策树都是判别模型。
- **生成模型**：是先计算联合概率分布然后通过贝叶斯公式转化为条件概率，朴素贝叶斯，隐马尔可夫模型是生成模型。
- **经验风险**：对**所有训练样本**都求一次损失函数，再累加求平均。即，模型f(x)f(x)对训练样本中所有样本的预测能力。
- **期望风险**：对**所有样本**（包含未知样本和已知的训练样本）的预测能力，是全局概念。（经验风险则是局部概念，仅仅表示决策函数对训练数据集里的样本的预测能力。）
- **结构风险**：对经验风险和期望风险的折中，在经验风险函数后面加一个正则化项（惩罚项），是一个大于0的系数λλ。J(f)J(f)表示的是模型的复杂度。

### 决策树

**优点**

​	1.可以很容易地将模型进行可视化

​	2.不需要对数据进行预处理

​	3.计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感

​	4.既能用于分类，也能用于回归

​	5.数据形式非常容易理解

**缺点**

​	不可避免会出现过拟合的问题，使得模型的泛化能力大打折扣

**注意**

​	**决策树：参数max_depth越大，越容易过拟合**



### 逻辑斯蒂回归

#### 原理（经常问）

​	1.找到一个合适的预测函数（分类函数）（sigmoid函数，也就逻辑斯蒂函数）
​	2.构造一个损失函数，预测的y值减去真实的y值，这个函数就是构造的损失函数。可以是二者的差，或者其他形式，综合Cost求和或者求平均，即为J(θ)
​	3.J(θ)的值越小就证明预测函数越正确，梯度下降法

#### 主要思想

​	利用Logistics回归进行分类的主要思想是：**根据现有数据对分类边界线建立回归公式**，**以此进行分类**。这里的“回归” 一词源于最佳拟合，表示要找到最佳拟合参数集。



#### 理解

<https://blog.csdn.net/touch_dream/article/details/79371462> （精辟）

​	逻辑斯蒂回归是一种对数线性模型。经典的逻辑斯蒂回归模型（LR）可以用来解决二分类问题，但是它输出的并不是确切类别，而是一个概率。     

​	在分析LR原理之前，先分析一下线性回归。线性回归能将输入数据通过对各个维度的特征分配不同的权重来进行表征，使得所有特征协同作出最后的决策。但是，这种表征方式是对模型的一个拟合结果，不能直接用于分类。

 	在LR中，将线性回归的结果通过sigmod函数映射到0到1之间，映射的结果刚好可以看做是数据样本点属于某一类的概率，如果结果越接近0或者1，说明分类结果的可信度越高。这样做不仅应用了线性回归的优势来完成分类任务，而且分类的结果是0~1之间的概率，可以据此对数据分类的结果进行打分。对于线性不可分的数据，可以对非线性函数进行线性加权，得到一个不是超平面的分割面。

#### 优缺点

**优点**

​	1.适合需要得到一个分类概率的场景。		

​	2.计算代价不高，容易理解实现。LR在时间和内存需求上相当高效。它可以应用于分布式数据，并且还有在线算法实现，用较少的资源处理大型数据。

​	3.LR对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响。（严重的多重共线性则可以使用逻辑回归结合L2正则化来解决，但是若要得到一个简约模型，L2正则化并不是最好的选择，因为它建立的模型涵盖了全部的特征。）

   **缺点**

​	1.容易欠拟合，分类精度不高。

​	2.数据特征有缺失或者特征空间很大时表现效果并不好。

### 逻辑回归和线性回归的区别

​	其实没有多大的区别，就是逻辑回归本质是线性回归，不过多了一个Sigmoid函数，使样本能映射到[0,1]之间的数值，用来做分类问题。

线性回归用来预测，逻辑回归用来分类。

​          线性回归是拟合函数，逻辑回归是预测函数

​          线性回归的参数计算方法是最小二乘法，逻辑回归的参数计算方法是梯度下降

### 逻辑斯蒂回归与SVM比较

​	线性回归做分类因为考虑了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大；LR和SVM克服了这个缺点，其中LR将所有数据采用sigmod函数进行了非线性映射，使得远离分类决策面的数据作用减弱；SVM直接去掉了远离分类决策面的数据，只考虑支持向量的影响。

  	但是对于这两种算法来说，在线性分类情况下，如果异常点较多无法剔除的话，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献；SVM+软间隔对异常比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。



### 决策树的减枝

#### 算法目的

​	决策树的剪枝是为了简化决策树模型，避免过拟合。

#### 算法基本思路

​	剪去决策树模型中的一些子树或者叶结点，并将其上层的根结点作为新的叶结点，从而减少了叶结点甚至减少了层数，降低了决策树复杂度。

#### 剪枝类型

​	**预剪枝、后剪枝**	

- 预剪枝是在决策树生成过程中，对树进行剪枝，提前结束树的分支生长。
- 后剪枝是在决策树生长完成之后，对树进行剪枝，得到简化版的决策树。

#### **预剪枝依据**

- 作为叶结点或作为根结点需要含的最少样本个数
- 决策树的层数
- 结点的经验熵小于某个阈值才停止

决策树的减枝：https://blog.csdn.net/bird_fly_i/article/details/72824639



### K-近邻（KNN）

**可以用于分类，也可以用于回归**

分类：类别是明显的

回归：对趋势的一个预测（预测走势），是一个范围

#### k-近邻算法原理

简单地说，K-近邻算法采用测量不同特征值之间的距离方法进行分类。

- 优点：精度高、对异常值不敏感、无数据输入假定。
- 缺点：时间复杂度高、空间复杂度高。
- 适用数据范围：数值型和标称型。

### 支持向量机SVM

#### 支持向量机的原理

**原理**

​	根据部分的数据决定分类器（分类边界线）。

**支持向量机**

​	其含义是通过支持向量运算的分类器。其中“机”的意思是机器，可以理解为分类器。 

**支持向量**

​	在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。

#### 解决的问题

- 线性分类

  在训练数据中，每个数据都有n个的属性和一个二类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面（hyperplane），这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。 其实这样的超平面有很多，我们要找到一个最佳的。因此，增加一个约束条件：**这个超平面到每边最近数据点的距离是最大的**。也成为最大间隔超平面（maximum-margin hyperplane）。这个分类器也成为最大间隔分类器（maximum-margin classifier）。 支持向量机是一个二类分类器。

- 非线性分类

  SVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法和KKT条件，以及核函数可以产生非线性分类器。 

#### 理解

​	SVM的目的是要找到一个线性分类的最佳超平面 f(x)=xw+b=0。求 w 和 b。

​	首先通过两个分类的最近点，找到f(x)的约束条件。

​	有了约束条件，就可以通过拉格朗日乘子法和KKT条件来求解，这时，问题变成了求拉格朗日乘子αi 和 b。

​	对于异常点的情况，加入松弛变量ξ来处理。

​	非线性分类的问题：映射到高维度、使用核函数。

### 贝叶斯优缺点

**优点**

- 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率；
- 对小规模的数据表现很好；
- 能处理多分类任务，适合增量式训练；
- 对缺失数据不太敏感，算法也比较简单，常用于文本分类

**缺点**

- 只能用于分类问题
- 需要计算先验概率；
- 分类决策存在错误率；
- 对输入数据的表达形式很敏感

### 为什么说朴素贝叶斯是高偏差低方差？

​	它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会偏差部分大于方差部分，也就是高偏差，低方差。

### 随机森林如何评估特征重要性（重要）

https://blog.csdn.net/zjupeco/article/details/77371645

http://charleshm.github.io/2016/03/Random-Forest-Tricks/

衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：
​	1) Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。
​	2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。

### 傅里叶（理解）

https://blog.csdn.net/Murray_/article/details/80277083

1.以时间作为参照来观察动态世界的方法称为时域分析。时域分析中的一条曲线，在频域中只是一个频率值，或者一系列不同频率值得叠加（时域图中横坐标为时间，频域图中横坐标为频率）

2.任何波形都可以由正弦波叠加而来。

3.傅里叶变换，就是将一个时域非周期的连续信号，转换为一个在频域上非周期的连续信号

### 交叉验证

https://www.cnblogs.com/pinard/p/5992719.html

### 集成学习

https://www.cnblogs.com/pinard/p/6131423.html

#### 1.概述

​	对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。

​	也就是说，集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。

​	也就是说，集成学习有两个主要的问题需要解决。

​	**第一是如何得到若干个个体学习器**；

​	**第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器**。

#### 2.集成学习之个体学习器

​	集成学习的第一个问题就是如何得到若干个个体学习器。这里我们有两种选择。

　　第一种就是所有的个体学习器都是一个种类的，或者说是同质的。比如都是决策树个体学习器，或者都是神经网络个体学习器。第二种是所有的个体学习器不全是一个种类的，或者说是异质的。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。

　　目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。下面就分别对这两类算法做一个概括总结。

#### 3.集成学习之boosting

​	boosting的算法原理我们可以用一张图做一个概括如下：

​	看链接

​	从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

　　Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。AdaBoost和提升树算法的原理在后面的文章中会专门来讲。

#### 4.集成学习之bagging

​	Bagging的算法原理和 boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：

​	看链接

从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。

　　对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

　　随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。

#### 5. 集成学习之结合策略

　　在上面几节里面我们主要关注于学习器，提到了学习器的结合策略但没有细讲，本节就对集成学习之结合策略做一个总结。我们假定我得到的T个弱学习器是{h1,h2,...hT}

##### 5.1平均法

##### 5.2投票法

##### 5.3学习法

### 交叉验证与网格搜索

待写

### 标准化与归一化的区别

​	简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。

算法可视化：https://visualgo.net/en